

#################################
############ MAPPING ############
#################################

#choose the genome you're working with
source /work/02260/grovesd/lonestar/myReferences/humanReference.sh   #human
source /work/02260/grovesd/lonestar/myReferences/chimpReference.sh   #chimp
source /work/02260/grovesd/lonestar/myReferences/macaqueRference.sh  #macaque


#check genome load
echo $GENOME_PATH
head $GENOME_PATH

module load bowtie
>mapse
for file in *_1.trim
do echo "\
bowtie2 -x $GENOME_PATH -U ${file} --local -p 8 -S ${file/_1.trim/}.sam">> mapse
done

launcher_creator.py -n mapRNAmacac -j mapse -q normal -N 4 -w 4 -a $allo -t 24:00:00
#with -p 6 for bowtie, wayness of 6 seems good (36 total out of 48, leaving some extra memory), assuming 12 pairs of .trim files to map, 2 nodes lets them all map at once
#This sometimes finishes as fast as a couple hours, but can take much longer for large .trim files, could be fancy to set the amount of time requested based on the read counts




###########################################
##### PREPARE ALIGNMENTS FOR COUNTING #####
###########################################

#SORT BY COORDIANTE, REMOVE DUPLICATES, THEN CONVERT BACK TO SAM FOR COUNTING

###!!! NOTE! REMOVING PCR DUPLICATES MAY DO MORE HARM THAN GOOD:
#(https://dnatech.genomecenter.ucdavis.edu/faqs/should-i-remove-pcr-duplicates-from-my-rna-seq-data/)
#https://www.biostars.org/p/55648/   and these excellent papers
#Parekh et al 2016:  The impact of amplification on differential expression analyses by RNA-seq.  and
#Fu et al. 2018:  Elimination of PCR duplicates in RNA-seq and small RNA-seq using unique molecular identifiers.
# THE STEPS HERE ARE LEFT FOR REFERENCE, BUT MAYBE DON'T TO DO THIS?
# INSTEAD, JUST SORT THEM AND GO STRAIGHT TO FEATURECOUNTS


#These are fast, but removing duplicates takes a lot of memory

module load samtools
>removeDups
for file in *.sam
do runID=${file/.sam/}
 echo "samtools sort -O bam -o ${runID}_sorted.bam $file &&\
 java -Xms4g -jar /work/02260/grovesd/lonestar/picard/picard-tools-1.119/MarkDuplicates.jar\
 INPUT=${runID}_sorted.bam\
 OUTPUT=${runID}_dupsRemoved.bam\
 METRICS_FILE=${runID}_dupMetrics.txt\
 REMOVE_DUPLICATES=true &&\
 samtools index ${runID}_dupsRemoved.bam" >> removeDups
 done
 
launcher_creator.py -n rDupHumanTissue -j removeDups -t 24:00:00 -q normal -a $allo -e $email -N 4 -w 2
##run only two per node, assuming 12 sam files, using six nodes allows for all to run simultaneously, expect run to last ~2-4 hours


######################################
############# GET COUNTS #############
######################################




#choose the genome you're working with
source /work/02260/grovesd/lonestar/myReferences/humanReference.sh   #human
source /work/02260/grovesd/lonestar/myReferences/chimpReference.sh   #chimp
source /work/02260/grovesd/lonestar/myReferences/macaqueRference.sh  #macaque


#rename the dupsRemoved bams for easier upload into R
for file in *.bam*; do mv $file human${file};done #for human
for file in *.bam*; do mv $file chimp${file};done #for chimp
for file in *.bam*; do mv $file macac${file};done #for macac



#run featurecounts
echo "featureCounts -a $GFF_PATH -p -t gene -g $GENE_ID -o feature_counts_out.tsv -T 48 --primary *_dupsRemoved.bam" > runFeatureCounts





#fix names eg:
#add spp name to front of each name

#######################################
####### PIPELINE COUNTS RESULTS #######
#######################################


wc -l *.fastq |\
 awk '{split($2, a, ".fastq")
 print a[1]"\t"$1/4"\trawCounts"}' |\
 grep -v total > raw_read_counts.tsv &




#GET POST TRIMMING READ COUNT
wc -l *.trim |\
 awk '{split($2, a, ".trim")
 print a[1]"\t"$1/4"\ttrimmedCounts"}' |\
 grep -v total > trimmed_read_counts.tsv &


#get alignment counts before removal
>getInitialAlignment
for file in *sorted.bam
do echo "samtools flagstat $file > ${file/_sorted.bam/}_prededup_flagstats.txt &" >> getInitialAlignment
done

#get post removal alignment counts
>getDupRemAlignment
for file in *dupsRemoved.bam
do echo "samtools flagstat $file > ${file/.bam/}_post_dedup_flagstats.txt &" >> getDupRemAlignment
done



#format total reads
>prededup_mapped_count.tsv
for file in *prededup_flagstats.txt
do pp=$(grep "mapped" $file | head -n 1)
 echo -e "$file\t$pp" |\
 awk '{split($1, a, "_prededup_flagstats.txt")
 print a[1]"\t"$2"\tpredupMapped"}' >> prededup_mapped_count.tsv
 done



#format total reads
>dedup_mapped_count.tsv
for file in *_post_dedup_flagstats.txt
do pp=$(grep "mapped" $file | head -n 1)
 echo -e "$file\t$pp" |\
 awk '{split($1, a, "_dupsRemoved_post_dedup_flagstats.txt")
 print a[1]"\t"$2"\tdedupMapped"}' >> dedup_mapped_count.tsv
 done


#COUNTED ON GENES
total_gene_counts_featureCounts.R feature_counts_out.tsv



#ASSEMBLE
cat raw_read_counts.tsv trimmed_read_counts.tsv prededup_mapped_count.tsv dedup_mapped_count.tsv gene_count_sumsFC.tsv > pipelineCounts.tsv